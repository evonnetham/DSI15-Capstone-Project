{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Airbnb Price Listing Prediction\n",
    "## Part 4 Model Tuning\n",
    "\n",
    "_Authors: Evonne Tham_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, the XGBoost produced a high $R^2$ score of 0.967 and 0.749 for the train and validation sets respectively. Despite this, the model needs to be improve and fully leverage its advantages over other algorithms\n",
    "\n",
    "This will be done by utilising gridsearch after which we will be able to look into the the features importance, a built-in function in XGBoost,. This model will be used as the production model in the next notebook as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of this notebook\n",
    "- [1. Import Necessary Libraries and Load Data](#1.-Import-Necessary-Libraries-and-Load-Data)\n",
    "- [2. Model Prep](#2.-Model-Prep)\n",
    "- [3. GridSearch for Hyperparameter Tuning](#3.-GridSearch-for-Hyperparameter-Tuning)\n",
    "    - [3.1. Defining Function for modelling](#3.1-Defining-Function-for-modelling)\n",
    "    - [3.2. Fitting Models](#3.2.-Fitting-Models)\n",
    "- [4. Model Evaluation](#3.-Model-Evaluation)\n",
    "- [5. Re-training the Best Model (XGBoost)](#5.-Re-training-the-Best-Model-(XGBoost))\n",
    "- [6. Feature Importances](#6.1.-Feature-Importances)\n",
    "- [7.Evaluation of Final XGBoost Model on Test Set](#7.-Evaluation-of-Final-XGBoost-Model-on-Test-Set)\n",
    "    - [7.1. Model Prep](#7.1.-Model-Prep)\n",
    "    - [7.2. Model Evaluation](#7.2.-Model-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Saving models\n",
    "import pickle\n",
    "\n",
    "#Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Listing: 10564 | Total Number of Features: 133\n"
     ]
    }
   ],
   "source": [
    "# Load in Data \n",
    "train = pd.read_csv('../datasets/train.csv')\n",
    "\n",
    "#Set id as index \n",
    "train.set_index('id', inplace=True)\n",
    "\n",
    "print(f\"Total Number of Listing: {train.shape[0]} | Total Number of Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X = train[features]\n",
    "y = train['price']\n",
    "\n",
    "# Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. GridSearch for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining Function for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the important hyperparameters to tune an SVR are:\n",
    "- C: Regularization parameter\n",
    "- gamma: defines how much influence a single training example has\n",
    "- Kernel: helps find a hyperplane in the higher dimensional space without increasing the computational cost.\n",
    "\n",
    "Some of the important hyperparameters to tune an XGBoost are:\n",
    "\n",
    "- gamma: Specifies the minimum loss reduction required to make a split.\n",
    "- learning_rate: Rate at which our model learns patterns in data. After every round, it shrinks the feature weights to reach the best optimum.\n",
    "- max_depth: Determines how deeply each tree is allowed to grow during any boosting round.\n",
    "- n_estimators:  Number of trees one wants to build.\n",
    "- subsample: Ratio of the training instances which help in prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_modeller_val_scorer(classifier): \n",
    "    \n",
    "    '''\n",
    "    takes arguments \"lr\", \"enet\", \"svr\", \"xgb\"\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Model instantiation\n",
    "    clf_lr = LinearRegression()\n",
    "    clf_enet = ElasticNetCV()\n",
    "    clf_svr = SVR()\n",
    "    clf_xgb = XGBRegressor()\n",
    "    \n",
    "    # Building the model pipelines incl. preprocessing where needed \n",
    "    # Setting up the parameter grids\n",
    "    if classifier == \"lr\":\n",
    "        pipe_lr = Pipeline([('rs', RobustScaler()),\n",
    "                             ('clf_lr', clf_lr)])\n",
    "        \n",
    "        param_grid_lr = [{'clf_lr__fit_intercept': [True, False],\n",
    "                          'clf_lr__normalize': [True, False]}]\n",
    "        \n",
    "\n",
    "        gs = GridSearchCV(pipe_lr, \n",
    "                          param_grid_lr, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          verbose=1, \n",
    "                          scoring = \"r2\") \n",
    "        \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "    elif classifier == \"enet\":\n",
    "        pipe_enet = Pipeline([('rs', RobustScaler()), \n",
    "                             ('clf_enet', clf_enet)])\n",
    "        \n",
    "        param_grid_enet = [{'clf_enet__l1_ratio': [.1, .5, .7, .9, .95, .99, 1],\n",
    "                            'clf_enet__n_alphas': [1,10,100,1000,10000]}]\n",
    "    \n",
    "        gs = GridSearchCV(pipe_enet, \n",
    "                          param_grid_enet, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          verbose=1, \n",
    "                          scoring = \"r2\") \n",
    "    \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "    elif classifier == \"svr\":\n",
    "        pipe_svr = Pipeline([('rs', RobustScaler()),\n",
    "                             (\"clf_svr\", clf_svr)])\n",
    "\n",
    "        param_grid_svr = [{\"clf_svr__C\":[1,10], \n",
    "                          \"clf_svr__gamma\":[0.001, 0.01, 0.1, 1], \n",
    "                          \"clf_svr__kernel\":('linear', 'rbf')}]  \n",
    "        \n",
    "        gs = GridSearchCV(pipe_svr, \n",
    "                          param_grid_svr, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          verbose=1,\n",
    "                          scoring = \"r2\") \n",
    "        \n",
    "        gs.fit(X_train, y_train)   \n",
    "        \n",
    "    \n",
    "    elif classifier == \"xgb\":\n",
    "        pipe_xgb = Pipeline([('rs', RobustScaler()),\n",
    "                            (\"clf_xgb\",clf_xgb)])\n",
    "        \n",
    "        param_grid_xgb  = [{\n",
    "            'clf_xgb__gamma':[0, 0.3], \n",
    "            'clf_xgb__learning_rate': [0.05, 0.3], \n",
    "            'clf_xgb__max_depth':[2,3,5], \n",
    "            'clf_xgb__n_estimators': [1000], \n",
    "            'clf_xgb__subsample': [0.05, 0.3, 0.5]\n",
    "        }]\n",
    "                        \n",
    "        gs = GridSearchCV(pipe_xgb, \n",
    "                          param_grid_xgb, \n",
    "                          cv=5, \n",
    "                          n_jobs=-1, \n",
    "                          verbose=1,\n",
    "                          scoring = \"r2\") \n",
    "        \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "    end = time.time()\n",
    "        \n",
    "    #get scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    val_score = gs.score(X_val, y_val)\n",
    "    y_pred = gs.predict(X_val)\n",
    "    \n",
    "    #get R2, MSE Score, RMSE score\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = (mean_squared_error(y_val, y_pred))**0.5\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics_list= [train_score, val_score, gs.best_score_, r2, mse, rmse]\n",
    "    \n",
    "    # Print out total run time \n",
    "    print(f\"Time taken to run: {round((end - start)/60,1)} minutes\")\n",
    "    print('==================================================================================')\n",
    "    print('')\n",
    "\n",
    "    # print out accuracy, estimator and parameters from GridSearchCV\n",
    "    print(f'Best train accuracy score = {train_score}')\n",
    "    print(f'Best validation accuracy score = {val_score}')\n",
    "    print(f'Best grid search score = {gs.best_score_}')\n",
    "    print(f'R2 score = {r2}')\n",
    "    print(f'Mean Square Error = {mse}')\n",
    "    print(f\"Root mean squared error = {rmse}\")\n",
    "    print('==================================================================================')\n",
    "    print('')\n",
    "    \n",
    "    print(f'Best estimator = {gs.best_estimator_}')\n",
    "    print(f'Best parameters = {gs.best_params_}')\n",
    "    print('==================================================================================')\n",
    "    print('')\n",
    "    \n",
    "    print(f\"metrics list for {classifier}:\", metrics_list)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Fitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 0.1 minutes\n",
      "==================================================================================\n",
      "\n",
      "Best train accuracy score = 0.2663057708331079\n",
      "Best validation accuracy score = 0.22152686505427577\n",
      "Best grid search score = 0.24489417386193235\n",
      "R2 score = 0.22152686505427577\n",
      "Mean Square Error = 334217619.6560607\n",
      "Root mean squared error = 18281.6197218972\n",
      "==================================================================================\n",
      "\n",
      "Best estimator = Pipeline(memory=None,\n",
      "         steps=[('rs',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ('clf_lr',\n",
      "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "                                  normalize=False))],\n",
      "         verbose=False)\n",
      "Best parameters = {'clf_lr__fit_intercept': True, 'clf_lr__normalize': False}\n",
      "==================================================================================\n",
      "\n",
      "metrics list for lr: [0.2663057708331079, 0.22152686505427577, 0.24489417386193235, 0.22152686505427577, 334217619.6560607, 18281.6197218972]\n"
     ]
    }
   ],
   "source": [
    "grid_modeller_val_scorer(\"lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-300b36452386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_modeller_val_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"enet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bf1e146ed07f>\u001b[0m in \u001b[0;36mgrid_modeller_val_scorer\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m     47\u001b[0m                           scoring = \"r2\") \n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;31m# All LinearModelCV parameters except 'cv' are acceptable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mpath_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'l1_ratio'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0ml1_ratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l1_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# introspect the constructor arguments to find the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# to represent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0minit_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;31m# Consider the constructor parameters excluding 'self'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         parameters = [p for p in init_signature.parameters.values()\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2831\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0;32m-> 2833\u001b[0;31m                                         follow_wrapper_chains=follow_wrapped)\n\u001b[0m\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_signature_from_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_signature_is_builtin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[1;32m   2160\u001b[0m                                     \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_POSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                                     default=defaults[offset]))\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m     \u001b[0;31m# *args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2475\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_paramkind_descr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2478\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_modeller_val_scorer(\"enet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_modeller_val_scorer(\"svr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. Extreme Gradient Boosting Trees Regressor (\"XGB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_modeller_val_scorer(\"xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Evaluation\n",
    "The evaluation metrics used will be mean squared error (for loss) and r squared (for accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list= [\"train_score\", \"val_score\", \"gs.best_score_\", \"r2\", \"mse\", \"rmse\"]\n",
    "\n",
    "lr = [0.293311786792287, 0.23564362949069895, \n",
    "      0.26746736189109616, 0.23564362949069895, \n",
    "      328156946.27455, 18115.102712227443]\n",
    "    \n",
    "enet = [0.28757306981759556, 0.2389933366947643, \n",
    "        0.26938059293997063, 0.2389933366947643, \n",
    "        326718834.7739322, 18075.365411906125]\n",
    "\n",
    "svr = [0.060527530482557435, 0.04881783727855327, \n",
    "       0.05491951532110416, 0.04881783727855327, \n",
    "       408365843.3585253, 20208.063820131934]\n",
    "\n",
    "xgb = [0.9602609604179062, 0.7659233380537225, \n",
    "       0.7675232926907812, 0.7659233380537225, \n",
    "       100494854.9421371, 10024.712212434684]\n",
    "\n",
    "\n",
    "eval_data = [lr, enet, svr, xgb]\n",
    "\n",
    "column_names = metrics_list\n",
    "\n",
    "index = [\"Linear Regression\", \"ElasticNetCV\", \"Support Vector Regressor\", \"XGBoost\"]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=column_names, index=index)\n",
    "    \n",
    "eval_df = eval_df.round(decimals = 4)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this table and our particular output reproduced above, we can see that Support Vector Machine Regressor performed the worst as the train set and validation set has a $R^2$ less than 0.01. This handily beats the baseline model score of -0.00029, nevertheless.\n",
    "\n",
    "Linear Regression and ElasticNet performed poorly as well, eventhough there's not much difference between the $R^2$ score on train set and the validation set. \n",
    "\n",
    "Hence the clear winner is eXtreme Gradient Boosting. It has the highest $R^2$ of almost 0.80 on the entire validation set, which means that the model is able to account for almost 80% of the variance in the target variable. With a $RMSE$ score of 10024.70, what this means is that our model's prediction is on average off by 10024¥ in terms of predicting the property's price. This is pretty good for a preliminary model using regression techniques.\n",
    "\n",
    "***The best parameters are:*** \n",
    "- gamma: 0, \n",
    "- learning_rate: 0.05, \n",
    "- max_depth: 5, \n",
    "- n_estimators: 1000, \n",
    "- subsample: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Price Vs Log Price Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model is better at predicting price as it is or better when it is transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X = train[features]\n",
    "y = train['price']\n",
    "\n",
    "# Train/Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state = 42) \n",
    "\n",
    "# Scale \n",
    "rs = RobustScaler()\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_val_rs = rs.transform(X_val)\n",
    "\n",
    "# Instantiate Best Model\n",
    "xgb = XGBRegressor(gamma = 0,\n",
    "                   learning_rate = 0.05, \n",
    "                   max_depth = 5, \n",
    "                   n_estimators = 1000, \n",
    "                   subsample = 0.5)\n",
    "\n",
    "# Fit Model\n",
    "xgb.fit(X_train_rs, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = xgb.predict(X_train_rs)\n",
    "y_pred_val = xgb.predict(X_val_rs)\n",
    "\n",
    "# Model Evaluation\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "print(f\"R2 Score on Training Set: {round(r2_train, 4)}\")\n",
    "print(f\"R2 Score on Validation Set: {round(r2_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE Score on Training Set: {round(mse_train, 4)}\")\n",
    "print(f\"MSE Score on Validation Set: {round(mse_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "print(f\"RMSE Score on Training Set: {round(rmse_train, 4)}\")\n",
    "print(f\"RMSE Score on Validation Set: {round(rmse_val, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it has a $R^2$ of 0.78 and a $RMSE$ score of 9903.10 on the validation set. hence we can proceed futher with the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Log Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X = train[features]\n",
    "y = train['price']\n",
    "\n",
    "# Train/Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state = 42) \n",
    "\n",
    "# Scale \n",
    "rs = RobustScaler()\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_val_rs = rs.transform(X_val)\n",
    "\n",
    "# Instantiate Best Model\n",
    "xgb = XGBRegressor(gamma = 0,\n",
    "                   learning_rate = 0.05, \n",
    "                   max_depth = 5, \n",
    "                   n_estimators = 1000, \n",
    "                   subsample = 0.5)\n",
    "\n",
    "# Fit Model\n",
    "xgb.fit(X_train_rs, np.log(y_train))\n",
    "\n",
    "# Predict\n",
    "y_pred_train = np.exp(xgb.predict(X_train_rs))\n",
    "y_pred_val = np.exp(xgb.predict(X_val_rs))\n",
    "\n",
    "# Model Evaluation\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "print(f\"R2 Score on Training Set: {round(r2_train, 4)}\")\n",
    "print(f\"R2 Score on Validation Set: {round(r2_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE Score on Training Set: {round(mse_train, 4)}\")\n",
    "print(f\"MSE Score on Validation Set: {round(mse_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "print(f\"RMSE Score on Training Set: {round(rmse_train, 4)}\")\n",
    "print(f\"RMSE Score on Validation Set: {round(rmse_val, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, XGBoost model got a worse performance when the target variable is being transformed. This is because regression trees make splits in a way that minimizes the MSE, which means that they minimize the sum of the variances of the target in the children nodes. Hence, when variable is skewed, high values will affect the variances and push the split points towards higher values - forcing the decision tree to make less balanced splits and trying to \"isolate\" the tail from the rest of the points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from its superior performance, a benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model.\n",
    "\n",
    "Generally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.\n",
    "\n",
    "This importance is calculated explicitly for each attribute in the dataset, allowing attributes to be ranked and compared to each other.\n",
    "\n",
    "Importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity (Gini index) used to select the split points or another more specific error function.\n",
    "\n",
    "The feature importances are then averaged across all of the the decision trees within the model.\n",
    "\n",
    "Credit: [Feature Importance and Feature Selection With XGBoost in Python](https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing top features in our production model. \n",
    "key_features = pd.Series(xgb.feature_importances_, index = X.columns)\n",
    "key_features.sort_values().plot(kind='barh', color = '#FF5A5F', figsize = (12,5))\n",
    "plt.xlabel(\"Feature importance\", fontsize=20);\n",
    "\n",
    "#nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About a quiet a number offeatures have a very low feature importance or are of of 0 in this XGBoost regression model. And here are the top 10 most important features:\n",
    "1. Room Type: Entire home/apt\n",
    "2. Room Type: Shared room\n",
    "3. Number of guests listing can Accommodate \n",
    "4. Whether there's Elevator\n",
    "5. Property Type: Hotel\n",
    "6. Number of Guests Included\n",
    "7. The number of Bathrooms\n",
    "8. The Maximum Night Stay \n",
    "9. Instant Bookable\n",
    "10. Cleaning Fee\n",
    "\n",
    "As expected the most important features being the room type which came out to be the entire flat. Listings are priced higher if the offer is for the entire flat/house. This could also suggest that offering the flat/house as a whole, rather than each bedroom individually, may be better overall, given the large difference in importance compared to the second most important feature, which is room type as well.\n",
    "\n",
    "The third highest is how many people the property accommodates, which explains as that's one of the main things one would use to filter when searching for properties with in the first place.\n",
    "\n",
    "However, it is surprising that location or distance features did not appear in the top ten list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation of Final XGBoost Model on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've selected the model with the best performance on the validation set and target variable, we're going to retrain the model on the combined train + validation sets using our best hyperparameter combination and see how well the model fair on the \"unseen\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data \n",
    "test = pd.read_csv('../datasets/test.csv')\n",
    "\n",
    "# Set id as index \n",
    "test.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X_train = train[features]\n",
    "y_train  = train['price']\n",
    "X_test = test[features]\n",
    "y_test = test['price']\n",
    "\n",
    "# Scale \n",
    "rs = RobustScaler()\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_test_rs = rs.transform(X_test)\n",
    "\n",
    "# Instantiate Best Model\n",
    "xgb = XGBRegressor(gamma = 0,\n",
    "                   learning_rate = 0.05, \n",
    "                   max_depth = 5, \n",
    "                   n_estimators = 1000, \n",
    "                   subsample = 0.5)\n",
    "\n",
    "# Model Fit\n",
    "xgb.fit(X_train_rs, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_test = xgb.predict(X_test_rs)\n",
    "\n",
    "# Model Evaluation\n",
    "r2_cross_val_score = np.mean(cross_val_score(xgb, X_train_rs, y_train, scoring=\"r2\"))\n",
    "train_score = xgb.score(X_train_rs, y_train)\n",
    "test_score = xgb.score(X_test_rs, y_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f\"R2 Score on Test Set: {round(r2, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"MSE Score on Test Set: {round(mse, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f\"RMSE Score on Test Set: {round(rmse, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "metrics_list= [train_score, test_score, r2_cross_val_score, r2, mse, rmse]\n",
    "print(f\"Metrics list for XGB on Test Set = {metrics_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our final XGB Model metrics into a DataFrame\n",
    "xgb_metrics_list= [\"train_score\", \"val_score\", \"gs.best_score_\", \"r2\", \"mse\", \"rmse\"]\n",
    "\n",
    "xgb_final = [0.9519852707369619, 0.8028979780337466, \n",
    "             0.7736988513281039, 0.8028979780337466, \n",
    "             95241018.86540219, 9759.150519661134]\n",
    "\n",
    "xgb_final_df = pd.DataFrame(xgb_final).T\n",
    "\n",
    "xgb_final_df.columns = xgb_metrics_list\n",
    "\n",
    "xgb_final_df.rename(index = {0:\"XGBoost (Test Set)\"}, inplace=True)\n",
    "\n",
    "# Stack the DataFrames on top of each other\n",
    "vertical_stack = pd.concat([eval_df, xgb_final_df], axis=0)\n",
    "\n",
    "vertical_stack = vertical_stack.round(decimals = 4)\n",
    "\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all our metrics above in the combined table, after training our XGB model with its ideal parameters onto the entire train set, the $R^2$ rises to 0.8029 from the initial validation $R^2$ to 0.7659. $RMSE$ has also decreased from 10024.7122 to 9759.1505\n",
    "\n",
    "The fact that $R^2$ score on the full recombined train set has a a higher score than the validation set $R^2$ score, as well as the lower RMSE Score, indicates that the model will generalize well on unseen data. Hence, we can now proceed to our production model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> Proceed to the next notebook for [Production Model](./05_Production_Model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
