{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Airbnb Price Listing Prediction\n",
    "## Part 4 Model Tuning\n",
    "\n",
    "_Authors: Evonne Tham_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, the XGBoost produced a high $R^2$ score of 0.967 and 0.749 for the train and validation sets respectively. Despite this, the model needs to be improve and fully leverage its advantages over other algorithms\n",
    "\n",
    "This will be done by utilising gridsearch after which we will be able to look into the the features importance, a built-in function in XGBoost,. This model will be used as the production model in the next notebook as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of this notebook\n",
    "- [1. Import Necessary Libraries and Load Data](#1.-Import-Necessary-Libraries-and-Load-Data)\n",
    "- [2. Model Prep](#2.-Model-Prep)\n",
    "- [3. GridSearch for Hyperparameter Tuning](#3.-GridSearch-for-Hyperparameter-Tuning)\n",
    "    - [3.1. Defining Function for modelling](#3.1-Defining-Function-for-modelling)\n",
    "    - [3.2. Fitting Models](#3.2.-Fitting-Models)\n",
    "- [4. Model Evaluation](#3.-Model-Evaluation)\n",
    "- [5. Re-training the Best Model (XGBoost)](#5.-Re-training-the-Best-Model-(XGBoost))\n",
    "- [6. Feature Importances](#6.1.-Feature-Importances)\n",
    "- [7.Evaluation of Final XGBoost Model on Test Set](#7.-Evaluation-of-Final-XGBoost-Model-on-Test-Set)\n",
    "    - [7.1. Model Prep](#7.1.-Model-Prep)\n",
    "    - [7.2. Fitting Model](#7.2.-Fitting-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, accuracy_score\n",
    "# from xgboost import plot_importance\n",
    "\n",
    "\n",
    "#Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Listing: 10564 | Total Number of Features: 134\n"
     ]
    }
   ],
   "source": [
    "# Load in Data \n",
    "train = pd.read_csv('../datasets/train.csv')\n",
    "\n",
    "#Set id as index \n",
    "train.set_index('id', inplace=True)\n",
    "\n",
    "print(f\"Total Number of Listing: {train.shape[0]} | Total Number of Features: {train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X = train[features]\n",
    "y = train['price']\n",
    "\n",
    "# Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. GridSearch for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining Function for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the important hyperparameters to tune an SVR are:\n",
    "- C: Regularization parameter\n",
    "- gamma: defines how much influence a single training example has\n",
    "- Kernel: helps find a hyperplane in the higher dimensional space without increasing the computational cost.\n",
    "\n",
    "Some of the important hyperparameters to tune an XGBoost are:\n",
    "\n",
    "- gamma: Specifies the minimum loss reduction required to make a split.\n",
    "- learning_rate: Rate at which our model learns patterns in data. After every round, it shrinks the feature weights to reach the best optimum.\n",
    "- max_depth: Determines how deeply each tree is allowed to grow during any boosting round.\n",
    "- n_estimators:  Number of trees one wants to build.\n",
    "- subsample: Ratio of the training instances which help in prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_modeller_val_scorer(classifier): \n",
    "    \n",
    "    '''\n",
    "    takes arguments \"lr\", \"enet\", \"svr\", \"xgb\"\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Model instantiation\n",
    "    clf_lr = LinearRegression()\n",
    "    clf_enet = ElasticNetCV()\n",
    "    clf_svr = SVR()\n",
    "    clf_xgb = XGBRegressor()\n",
    "    \n",
    "    # Building the model pipelines incl. preprocessing where needed \n",
    "    # Setting up the parameter grids\n",
    "    if classifier == \"lr\":\n",
    "        pipe_lr = Pipeline([('rs', RobustScaler()),\n",
    "                             ('clf_lr', clf_lr)])\n",
    "        \n",
    "        param_grid_lr = [{'clf_lr__fit_intercept': [True, False],\n",
    "                          'clf_lr__normalize': [True, False]}]\n",
    "        \n",
    "\n",
    "        gs = GridSearchCV(pipe_lr, \n",
    "                          param_grid_lr, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          verbose=1, \n",
    "                          scoring = \"r2\") \n",
    "        \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "    elif classifier == \"enet\":\n",
    "        pipe_enet = Pipeline([('rs', RobustScaler()), \n",
    "                             ('clf_enet', clf_enet)])\n",
    "        \n",
    "        param_grid_enet = [{'clf_enet__l1_ratio': [.1, .5, .7, .9, .95, .99, 1],\n",
    "                            'clf_enet__n_alphas': [1,10,100,1000,10000]}]\n",
    "    \n",
    "        gs = GridSearchCV(pipe_enet, \n",
    "                          param_grid_enet, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          verbose=1, \n",
    "                          scoring = \"r2\") \n",
    "    \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "    elif classifier == \"svr\":\n",
    "        pipe_svr = Pipeline([('rs', RobustScaler()),\n",
    "                             (\"clf_svr\", clf_svr)])\n",
    "\n",
    "        param_grid_svr = [{\"clf_svr__C\":[1,10], \n",
    "                          \"clf_svr__gamma\":[0.001, 0.01, 0.1, 1], \n",
    "                          \"clf_svr__kernel\":('linear', 'rbf')}]  \n",
    "        \n",
    "        gs = GridSearchCV(pipe_svr, \n",
    "                          param_grid_svr, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          verbose=1,\n",
    "                          scoring = \"r2\") \n",
    "        \n",
    "        gs.fit(X_train, y_train)   \n",
    "        \n",
    "    \n",
    "    elif classifier == \"xgb\":\n",
    "        pipe_xgb = Pipeline([('rs', RobustScaler()),\n",
    "                            (\"clf_xgb\",clf_xgb)])\n",
    "        \n",
    "        param_grid_xgb  = [{\n",
    "            'clf_xgb__gamma':[0, 0.3], \n",
    "            'clf_xgb__learning_rate': [0.05, 0.3], \n",
    "            'clf_xgb__max_depth':[2,3,5], \n",
    "            'clf_xgb__n_estimators': [1000], \n",
    "            'clf_xgb__subsample': [0.05, 0.3, 0.5]\n",
    "        }]\n",
    "                        \n",
    "        gs = GridSearchCV(pipe_xgb, \n",
    "                          param_grid_xgb, \n",
    "                          cv=5, \n",
    "                          n_jobs=-1, \n",
    "                          verbose=1,\n",
    "                          scoring = \"r2\") \n",
    "        \n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "    end = time.time()\n",
    "        \n",
    "    #get scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    val_score = gs.score(X_val, y_val)\n",
    "    y_pred = gs.predict(X_val)\n",
    "    \n",
    "    #get R2, MSE Score, RMSE score\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = (mean_squared_error(y_val, y_pred))**0.5\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics_list= [train_score, val_score, gs.best_score_, r2, mse, rmse]\n",
    "    \n",
    "    # Print out total run time \n",
    "    print(f\"Time taken to run: {round((end - start)/60,1)} minutes\")\n",
    "    print('==================================================================================')\n",
    "    print('')\n",
    "\n",
    "    # print out accuracy, estimator and parameters from GridSearchCV\n",
    "    print(f'Best train accuracy score = {train_score}')\n",
    "    print(f'Best validation accuracy score = {val_score}')\n",
    "    print(f'Best grid search score = {gs.best_score_}')\n",
    "    print(f'R2 score = {r2}')\n",
    "    print(f'Mean Square Error = {mse}')\n",
    "    print(f\"Root mean squared error = {rmse}\")\n",
    "    print('==================================================================================')\n",
    "    print('')\n",
    "    \n",
    "    print(f'Best estimator = {gs.best_estimator_}')\n",
    "    print(f'Best parameters = {gs.best_params_}')\n",
    "    print('==================================================================================')\n",
    "    print('')\n",
    "    \n",
    "    print(f\"metrics list for {classifier}:\", metrics_list)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Fitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 0.0 minutes\n",
      "==================================================================================\n",
      "\n",
      "Best train accuracy score = 0.2663057708331079\n",
      "Best validation accuracy score = 0.22152686505427577\n",
      "Best grid search score = 0.24489417386193235\n",
      "R2 score = 0.22152686505427577\n",
      "Mean Square Error = 334217619.6560607\n",
      "Root mean squared error = 18281.6197218972\n",
      "==================================================================================\n",
      "\n",
      "Best estimator = Pipeline(memory=None,\n",
      "         steps=[('rs',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ('clf_lr',\n",
      "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "                                  normalize=False))],\n",
      "         verbose=False)\n",
      "Best parameters = {'clf_lr__fit_intercept': True, 'clf_lr__normalize': False}\n",
      "==================================================================================\n",
      "\n",
      "metrics list for lr: [0.2663057708331079, 0.22152686505427577, 0.24489417386193235, 0.22152686505427577, 334217619.6560607, 18281.6197218972]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "grid_modeller_val_scorer(\"lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 175 out of 175 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 12.1 minutes\n",
      "==================================================================================\n",
      "\n",
      "Best train accuracy score = 0.2612188269507306\n",
      "Best validation accuracy score = 0.22410697347213326\n",
      "Best grid search score = 0.2483961607667235\n",
      "R2 score = 0.22410697347213326\n",
      "Mean Square Error = 333109915.7994196\n",
      "Root mean squared error = 18251.29901676644\n",
      "==================================================================================\n",
      "\n",
      "Best estimator = Pipeline(memory=None,\n",
      "         steps=[('rs',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ('clf_enet',\n",
      "                 ElasticNetCV(alphas=None, copy_X=True, cv='warn', eps=0.001,\n",
      "                              fit_intercept=True, l1_ratio=1, max_iter=1000,\n",
      "                              n_alphas=10000, n_jobs=None, normalize=False,\n",
      "                              positive=False, precompute='auto',\n",
      "                              random_state=None, selection='cyclic', tol=0.0001,\n",
      "                              verbose=0))],\n",
      "         verbose=False)\n",
      "Best parameters = {'clf_enet__l1_ratio': 1, 'clf_enet__n_alphas': 10000}\n",
      "==================================================================================\n",
      "\n",
      "metrics list for enet: [0.2612188269507306, 0.22410697347213326, 0.2483961607667235, 0.22410697347213326, 333109915.7994196, 18251.29901676644]\n"
     ]
    }
   ],
   "source": [
    "grid_modeller_val_scorer(\"enet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  9.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 9.9 minutes\n",
      "==================================================================================\n",
      "\n",
      "Best train accuracy score = 0.05696013672568723\n",
      "Best validation accuracy score = 0.04614121453535991\n",
      "Best grid search score = 0.05151951923907422\n",
      "R2 score = 0.04614121453535991\n",
      "Mean Square Error = 409514983.1833822\n",
      "Root mean squared error = 20236.47655060985\n",
      "==================================================================================\n",
      "\n",
      "Best estimator = Pipeline(memory=None,\n",
      "         steps=[('rs',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ('clf_svr',\n",
      "                 SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
      "                     gamma=0.001, kernel='linear', max_iter=-1, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Best parameters = {'clf_svr__C': 10, 'clf_svr__gamma': 0.001, 'clf_svr__kernel': 'linear'}\n",
      "==================================================================================\n",
      "\n",
      "metrics list for svr: [0.05696013672568723, 0.04614121453535991, 0.05151951923907422, 0.04614121453535991, 409514983.1833822, 20236.47655060985]\n"
     ]
    }
   ],
   "source": [
    "grid_modeller_val_scorer(\"svr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. Extreme Gradient Boosting Trees Regressor (\"XGB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 25.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 25.4 minutes\n",
      "==================================================================================\n",
      "\n",
      "Best train accuracy score = 0.9606509673173325\n",
      "Best validation accuracy score = 0.767056362326614\n",
      "Best grid search score = 0.7654563154065124\n",
      "R2 score = 0.767056362326614\n",
      "Mean Square Error = 100008419.818689\n",
      "Root mean squared error = 10000.420982073156\n",
      "==================================================================================\n",
      "\n",
      "Best estimator = Pipeline(memory=None,\n",
      "         steps=[('rs',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ('clf_xgb',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "                              importance_type='gain',\n",
      "                              interaction_constraints='', learning_rate=0.05,\n",
      "                              max_delta_step=0, max_depth=5, min_child_weight=1,\n",
      "                              missing=nan, monotone_constraints='()',\n",
      "                              n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n",
      "                              objective='reg:squarederror', random_state=0,\n",
      "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                              subsample=0.5, tree_method='exact',\n",
      "                              validate_parameters=1, verbosity=None))],\n",
      "         verbose=False)\n",
      "Best parameters = {'clf_xgb__gamma': 0, 'clf_xgb__learning_rate': 0.05, 'clf_xgb__max_depth': 5, 'clf_xgb__n_estimators': 1000, 'clf_xgb__subsample': 0.5}\n",
      "==================================================================================\n",
      "\n",
      "metrics list for xgb: [0.9606509673173325, 0.767056362326614, 0.7654563154065124, 0.767056362326614, 100008419.818689, 10000.420982073156]\n"
     ]
    }
   ],
   "source": [
    "grid_modeller_val_scorer(\"xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Evaluation\n",
    "The evaluation metrics used will be mean squared error (for loss) and r squared (for accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>gs.best_score_</th>\n",
       "      <th>r2</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>3.281569e+08</td>\n",
       "      <td>18115.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>0.2876</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>3.267188e+08</td>\n",
       "      <td>18075.3654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Regressor</th>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>4.083658e+08</td>\n",
       "      <td>20208.0638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.9603</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>0.7675</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>1.004949e+08</td>\n",
       "      <td>10024.7122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          train_score  val_score  gs.best_score_      r2  \\\n",
       "Linear Regression              0.2933     0.2356          0.2675  0.2356   \n",
       "ElasticNetCV                   0.2876     0.2390          0.2694  0.2390   \n",
       "Support Vector Regressor       0.0605     0.0488          0.0549  0.0488   \n",
       "XGBoost                        0.9603     0.7659          0.7675  0.7659   \n",
       "\n",
       "                                   mse        rmse  \n",
       "Linear Regression         3.281569e+08  18115.1027  \n",
       "ElasticNetCV              3.267188e+08  18075.3654  \n",
       "Support Vector Regressor  4.083658e+08  20208.0638  \n",
       "XGBoost                   1.004949e+08  10024.7122  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_list= [\"train_score\", \"val_score\", \"gs.best_score_\", \"r2\", \"mse\", \"rmse\"]\n",
    "\n",
    "lr = [0.293311786792287, 0.23564362949069895, \n",
    "      0.26746736189109616, 0.23564362949069895, \n",
    "      328156946.27455, 18115.102712227443]\n",
    "    \n",
    "enet = [0.28757306981759556, 0.2389933366947643, \n",
    "        0.26938059293997063, 0.2389933366947643, \n",
    "        326718834.7739322, 18075.365411906125]\n",
    "\n",
    "svr = [0.060527530482557435, 0.04881783727855327, \n",
    "       0.05491951532110416, 0.04881783727855327, \n",
    "       408365843.3585253, 20208.063820131934]\n",
    "\n",
    "xgb = [0.9602609604179062, 0.7659233380537225, \n",
    "       0.7675232926907812, 0.7659233380537225, \n",
    "       100494854.9421371, 10024.712212434684]\n",
    "\n",
    "\n",
    "eval_data = [lr, enet, svr, xgb]\n",
    "\n",
    "column_names = metrics_list\n",
    "\n",
    "index = [\"Linear Regression\", \"ElasticNetCV\", \"Support Vector Regressor\", \"XGBoost\"]\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data, columns=column_names, index=index)\n",
    "    \n",
    "eval_df = eval_df.round(decimals = 4)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this table and our particular output reproduced above, we can see that Support Vector Machine Regressor performed the worst as the train set and validation set has a $R^2$ less than 0.01. This handily beats the baseline model score of -0.00029, nevertheless.\n",
    "\n",
    "Linear Regression and ElasticNet performed poorly as well, eventhough there's not much difference between the $R^2$ score on train set and the validation set. \n",
    "\n",
    "Hence the clear winner is eXtreme Gradient Boosting. It has the highest $R^2$ of almost 0.80 on the entire validation set, which means that the model is able to account for almost 80% of the variance in the target variable. With a $RMSE$ score of 10024.70, what this means is that our model's prediction is on average off by 10024Â¥ in terms of predicting the property's price. This is pretty good for a preliminary model using regression techniques.\n",
    "\n",
    "***The best parameters are:*** \n",
    "- gamma: 0, \n",
    "- learning_rate: 0.05, \n",
    "- max_depth: 5, \n",
    "- n_estimators: 1000, \n",
    "- subsample: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Price Vs Log Price Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our model is better at predicting price as it is or better when it is transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score on Training Set: 0.9607\n",
      "R2 Score on Validation Set: 0.7671\n",
      "===============================================\n",
      "MSE Score on Training Set: 17193559.3107\n",
      "MSE Score on Validation Set: 100008419.8187\n",
      "===============================================\n",
      "RMSE Score on Training Set: 4146.5117\n",
      "RMSE Score on Validation Set: 10000.421\n"
     ]
    }
   ],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X = train[features]\n",
    "y = train['price']\n",
    "\n",
    "# Train/Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state = 42) \n",
    "\n",
    "# Scale \n",
    "rs = RobustScaler()\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_val_rs = rs.transform(X_val)\n",
    "\n",
    "# Instantiate Best Model\n",
    "xgb = XGBRegressor(gamma = 0,\n",
    "                   learning_rate = 0.05, \n",
    "                   max_depth = 5, \n",
    "                   n_estimators = 1000, \n",
    "                   subsample = 0.5)\n",
    "\n",
    "# Fit Model\n",
    "xgb.fit(X_train_rs, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = xgb.predict(X_train_rs)\n",
    "y_pred_val = xgb.predict(X_val_rs)\n",
    "\n",
    "# Model Evaluation\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "print(f\"R2 Score on Training Set: {round(r2_train, 4)}\")\n",
    "print(f\"R2 Score on Validation Set: {round(r2_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE Score on Training Set: {round(mse_train, 4)}\")\n",
    "print(f\"MSE Score on Validation Set: {round(mse_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "print(f\"RMSE Score on Training Set: {round(rmse_train, 4)}\")\n",
    "print(f\"RMSE Score on Validation Set: {round(rmse_val, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it has a $R^2$ of 0.78 and a $RMSE$ score of 9903.10 on the validation set. hence we can proceed futher with the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Log Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score on Training Set: 0.9162\n",
      "R2 Score on Validation Set: 0.7156\n",
      "===============================================\n",
      "MSE Score on Training Set: 36609909.4929\n",
      "MSE Score on Validation Set: 122092938.9678\n",
      "===============================================\n",
      "RMSE Score on Training Set: 6050.6123\n",
      "RMSE Score on Validation Set: 11049.5674\n"
     ]
    }
   ],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X = train[features]\n",
    "y = train['price']\n",
    "\n",
    "# Train/Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                  y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state = 42) \n",
    "\n",
    "# Scale \n",
    "rs = RobustScaler()\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_val_rs = rs.transform(X_val)\n",
    "\n",
    "# Instantiate Best Model\n",
    "xgb = XGBRegressor(gamma = 0,\n",
    "                   learning_rate = 0.05, \n",
    "                   max_depth = 5, \n",
    "                   n_estimators = 1000, \n",
    "                   subsample = 0.5)\n",
    "\n",
    "# Fit Model\n",
    "xgb.fit(X_train_rs, np.log(y_train))\n",
    "\n",
    "# Predict\n",
    "y_pred_train = np.exp(xgb.predict(X_train_rs))\n",
    "y_pred_val = np.exp(xgb.predict(X_val_rs))\n",
    "\n",
    "# Model Evaluation\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "print(f\"R2 Score on Training Set: {round(r2_train, 4)}\")\n",
    "print(f\"R2 Score on Validation Set: {round(r2_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "print(f\"MSE Score on Training Set: {round(mse_train, 4)}\")\n",
    "print(f\"MSE Score on Validation Set: {round(mse_val, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "print(f\"RMSE Score on Training Set: {round(rmse_train, 4)}\")\n",
    "print(f\"RMSE Score on Validation Set: {round(rmse_val, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, XGBoost model got a worse performance when the target variable is being transformed. This is because regression trees make splits in a way that minimizes the MSE, which means that they minimize the sum of the variances of the target in the children nodes. Hence, when variable is skewed, high values will affect the variances and push the split points towards higher values - forcing the decision tree to make less balanced splits and trying to \"isolate\" the tail from the rest of the points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 6. Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from its superior performance, a benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model.\n",
    "\n",
    "Generally, importance provides a score that indicates how useful or valuable each feature was in the construction of the boosted decision trees within the model. The more an attribute is used to make key decisions with decision trees, the higher its relative importance.\n",
    "\n",
    "This importance is calculated explicitly for each attribute in the dataset, allowing attributes to be ranked and compared to each other.\n",
    "\n",
    "Importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity (Gini index) used to select the split points or another more specific error function.\n",
    "\n",
    "The feature importances are then averaged across all of the the decision trees within the model.\n",
    "\n",
    "Credit: [Feature Importance and Feature Selection With XGBoost in Python](https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing top features in our production model. \n",
    "key_features = pd.Series(xgb.feature_importances_, index = X.columns)\n",
    "key_features.nlargest(15).sort_values().plot(kind='barh', color = '#FF5A5F', figsize = (12,5))\n",
    "plt.xlabel(\"Feature importance\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About a quiet a number offeatures have a very low feature importance or are of of 0 in this XGBoost regression model. And here are the top 10 most important features:\n",
    "1. Room Type: Entire home/apt\n",
    "2. Room Type: Shared room\n",
    "3. Number of guests listing can Accommodate \n",
    "4. Whether there's Elevator\n",
    "5. Property Type: Hotel\n",
    "6. Number of Guests Included\n",
    "7. The number of Bathrooms\n",
    "8. The Maximum Night Stay \n",
    "9. Instant Bookable\n",
    "10. Cleaning Fee\n",
    "\n",
    "As expected the most important features being the room type which came out to be the entire flat. Listings are priced higher if the offer is for the entire flat/house. This could also suggest that offering the flat/house as a whole, rather than each bedroom individually, may be better overall, given the large difference in importance compared to the second most important feature, which is room type as well.\n",
    "\n",
    "The third highest is how many people the property accommodates, which explains as that's one of the main things one would use to filter when searching for properties with in the first place.\n",
    "\n",
    "However, it is surprising that location or distance features did not appear in the top ten list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation of Final XGBoost Model on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've selected the model with the best performance on the validation set and target variable, we're going to retrain the model on the combined train + validation sets using our best hyperparameter combination and see how well the model fair on the \"unseen\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data \n",
    "test = pd.read_csv('../datasets/test.csv')\n",
    "\n",
    "# Set id as index \n",
    "test.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y variables\n",
    "features = [col for col in train._get_numeric_data().columns \n",
    "            if col != 'price' \n",
    "            and col != 'log_price' \n",
    "            and col != 'id' \n",
    "            and col != 'host_id']\n",
    "\n",
    "X_train = train[features]\n",
    "y_train  = train['price']\n",
    "X_test = test[features]\n",
    "y_test = test['price']\n",
    "\n",
    "# Scale \n",
    "rs = RobustScaler()\n",
    "X_train_rs = rs.fit_transform(X_train)\n",
    "X_test_rs = rs.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score on Test Set: 0.8029\n",
      "===============================================\n",
      "MSE Score on Test Set: 95241018.8654\n",
      "===============================================\n",
      "RMSE Score on Test Set: 9759.1505\n",
      "===============================================\n",
      "metrics list for XGB on Test Set = [0.9519852707369619, 0.8028979780337466, 0.7736988513281039, 0.8028979780337466, 95241018.86540219, 9759.150519661134]\n"
     ]
    }
   ],
   "source": [
    "# Fit Model\n",
    "xgb.fit(X_train_rs, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_test = xgb.predict(X_test_rs)\n",
    "\n",
    "# Model Evaluation\n",
    "r2_cross_val_score = np.mean(cross_val_score(xgb, X_train_rs, y_train, scoring=\"r2\"))\n",
    "train_score = xgb.score(X_train_rs, y_train)\n",
    "test_score = xgb.score(X_test_rs, y_test)\n",
    "\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f\"R2 Score on Test Set: {round(r2_test, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"MSE Score on Test Set: {round(mse_test, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f\"RMSE Score on Test Set: {round(rmse_test, 4)}\")\n",
    "print('===============================================')\n",
    "\n",
    "metrics_list= [train_score, test_score, r2_cross_val_score, r2, mse, rmse]\n",
    "print(f\"Metrics list for XGB on Test Set = {metrics_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>gs.best_score_</th>\n",
       "      <th>r2</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>3.281569e+08</td>\n",
       "      <td>18115.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>0.2876</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>3.267188e+08</td>\n",
       "      <td>18075.3654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Regressor</th>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>4.083658e+08</td>\n",
       "      <td>20208.0638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.9603</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>0.7675</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>1.004949e+08</td>\n",
       "      <td>10024.7122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost (Test Set)</th>\n",
       "      <td>0.9520</td>\n",
       "      <td>0.8029</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>0.8029</td>\n",
       "      <td>9.524102e+07</td>\n",
       "      <td>9759.1505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          train_score  val_score  gs.best_score_      r2  \\\n",
       "Linear Regression              0.2933     0.2356          0.2675  0.2356   \n",
       "ElasticNetCV                   0.2876     0.2390          0.2694  0.2390   \n",
       "Support Vector Regressor       0.0605     0.0488          0.0549  0.0488   \n",
       "XGBoost                        0.9603     0.7659          0.7675  0.7659   \n",
       "XGBoost (Test Set)             0.9520     0.8029          0.7737  0.8029   \n",
       "\n",
       "                                   mse        rmse  \n",
       "Linear Regression         3.281569e+08  18115.1027  \n",
       "ElasticNetCV              3.267188e+08  18075.3654  \n",
       "Support Vector Regressor  4.083658e+08  20208.0638  \n",
       "XGBoost                   1.004949e+08  10024.7122  \n",
       "XGBoost (Test Set)        9.524102e+07   9759.1505  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building our final XGB Model metrics into a DataFrame\n",
    "xgb_metrics_list= [\"train_score\", \"val_score\", \"gs.best_score_\", \"r2\", \"mse\", \"rmse\"]\n",
    "\n",
    "xgb_final = [0.9519852707369619, 0.8028979780337466, \n",
    "             0.7736988513281039, 0.8028979780337466, \n",
    "             95241018.86540219, 9759.150519661134]\n",
    "\n",
    "xgb_final_df = pd.DataFrame(xgb_final).T\n",
    "\n",
    "xgb_final_df.columns = xgb_metrics_list\n",
    "\n",
    "xgb_final_df.rename(index = {0:\"XGBoost (Test Set)\"}, inplace=True)\n",
    "\n",
    "# Stack the DataFrames on top of each other\n",
    "vertical_stack = pd.concat([eval_df, xgb_final_df], axis=0)\n",
    "\n",
    "vertical_stack = vertical_stack.round(decimals = 4)\n",
    "\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all our metrics above in the combined table, after training our XGB model with its ideal parameters onto the entire train set, the $R^2$ rises to 0.8029 from the initial validation $R^2$ to 0.7659. $RMSE$ has also decreased from 10024.7122 to 9759.1505\n",
    "\n",
    "The fact that $R^2$ score on the full recombined train set has a a higher score than the validation set $R^2$ score, as well as the lower RMSE Score, indicates that the model will generalize well on unseen data. Hence, we can now proceed to our production model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> Proceed to the next notebook for [Production Model](./05_Production_Model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
